# 一些面经
# 熟悉的
## 与Spark相关

1. spark rdd介绍一下
    RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据（计算）抽象。代码中是一个抽象类，它代表一个不可变、可分区、里面的元素可并行计算的集合。
2. spark的宽依赖和窄依赖
3. Spark任务执行流程
4. 谈谈MapReduce和Spark处理数据的区别

    [MapReduce和Spark处理数据的区别](https://blog.csdn.net/dashujuedu/article/details/53487199)
    
5. Spark的Stage是怎么划分的，如何优化。宽窄依赖。

7. Spark生态体系.和优化
8. 说对hadoop的理解，都有哪些组件，分别是干什么的
9. 说下spark中的transform和action
10. 为什么spark要把操作分为transform和action
11. spark中有了RDD，为什么还要有Dataframe和DataSet？

    DataFrame也是懒执行的。性能上比RDD要高，主要原因：优化的执行计划,查询计划通过`Spark catalyst optimiser`进行`filter`过滤掉无用的数据量后在进行`join`操作。
    比如：进行人口分析，可以先进行
    `Dataset`有用户友好的API风格，既具有类型安全检查也具有Dataframe的查询优化特性。`Dataframe`是`Dataset`的特列，`DataFrame=Dataset[Row]`。
12. spark - 自由发挥 能说多少说多少
13. Spark执行任务流程 容错机制 结构 RDD算子 宽窄依赖（具体举例） shuffle细节
14. sparkSql
    Spark SQL是Spark用来处理结构化数据的一个模块，它提供了2个编程抽象：DataFrame和DataSet，并且作为分布式SQL查询引擎的作用。
    我们已经学习了Hive，它是将Hive SQL转换成MapReduce然后提交到集群上执行，大大简化了编写MapReduc的程序的复杂性，由于MapReduce这种计算模型执行效率比较慢。所有Spark SQL的应运而生，它是将Spark SQL转换成RDD，然后提交到集群执行，执行效率非常快！
    
## Hive
1. Hive的作用
## HDFS
## 其他

1. Yarn
2. 编程说思路：1G文件，每行是一个词，内存1M，求词频最大的前100个词
3. **HDFS的NameNode怎么知道它手下有多少DataNode的。容错机制。副本存放策略。**
4. Yarn的架构
5. Yarn的Resource Manager, Node Manager,AppMaster等怎么工作的，有任务来的时候的工作流程
6. Spark的2种运行模式，standalone和cluster模式的区别，内部工作流程。
7. 了解函数式编程吗？说下c/c++和scala这种函数式编程语言的区别
8. Hadoop生态圈
9. 对Hive Hbase的了解 (简历项目里面用了什么)
10. Hadoop vs Spark (自由发挥 说到你说不出来)
11. 
# 模糊的
## 与Spark相关
1. `Spark`列式存储，`Spark`中`Join`连接

    [列式存储-CSDN](https://blog.csdn.net/yu616568/article/details/50993491)
    
    [列式存储一定快吗？--知乎](https://www.zhihu.com/question/29380943)
    
    [用图文解释列式存储--CSDN](https://blog.csdn.net/dc_726/article/details/41143175)
    
    [数据库中的 “行式存储”和“列式存储”---腾讯云社区](https://cloud.tencent.com/developer/article/1528525)
    
    1)作用：在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD

    3)需求：创建两个pairRDD，并将key相同的数据聚合到一个元组。

2. stage划分的依据是什么
3. 数据倾斜
4. Spark哪些部分可以优化
5. 解决Spark性能瓶颈问题
    - 基础调优
        提交参数设置；并行度设置；设置缓存/检查点；使用广播变量；Kryo序列化（重要）

    - Rdd算子优化
        RDD复用；尽早filter；读取大量小 文件用wholeTextFiles；mapPartition和foreachPartition；repartition调节并行度，也可以通过conf参数配置；本地预聚合，能用reduceBykey，就不用了groupBykey；

    - shuflle优化
        当数据量更多的时候，把map和reduce端的缓冲区大小调大，reduce端重试和等待时间间隔调大；bypass机制开启阈值

    - 数据倾斜优化
    - JVM 调优
## 与HDFS相关
1. hdfs小文件过多会怎么样

    [参考文章--CSDN](https://blog.csdn.net/SunnyYoona/article/details/53870077)
    
    [参考文章--腾讯云社区](https://cloud.tencent.com/developer/article/1512285)
    
    因为在hdfs 中，数据的元数据信息是保存在NameNode上的，hdfs本身的作用就是用来存储海量文件的，首先小文件过多的话，会增加NameNode 的压力，，因为NameNode是要接收集群中所有的DataNode的心跳信息，来确定元数据的信息变化的，另外，文件中可使用的block块的个数是有限制的，hadoop用来处理数据的话，小文件的延迟和数据量虽然很小，但是有些地方和大文件所耗的时间相同，所以最好做优化，避免这种情况的发生。

太多的小文件除了可能会撑爆NameNode，还会影响hive和spark计算速度。由于spark计算时会将数据从磁盘读取到内存，零碎的文件将产生较多的寻址过程。

2. spark的job,stage,task
3. DAGscheduler干了什么活
    `DGAScheduler`负责把`DAG`分解成多个阶段，每个阶段中包含了多个任务，每个任务都会被任务调度器分发给各个工作节点（`Worker Node`）上的`Executor`去执行。
   
## 其他组件
- Hive Hbase你的了解
- 
# 不了解的
## 与Spark相关
1. 用spark求一下dau
2. spark统计日活，日志文件每行是日期和user_id

    [spark 统计每天新增用户数](https://blog.csdn.net/dkl12/article/details/80256688)
    
3. **Spark的TaskScheduler是怎么分配task的，源码看过吗**

## 一些其他组件
1. kafka在什么地方需要用到zookeeper
2. 了解HBase吗？
3. **给一个日志文件，有用户ID，时间戳，url，用mapreduce如何给出每天每个站点的访问任务**
4. Kafuka原理
5. Hadoop MR全部过程
6. **大表join优化方法**
# 其他
## 数据仓库与数据库

[数据库与数据仓库的本质区别是什么?---知乎](https://www.zhihu.com/question/20623931)

1. 100亿个数怎么求中位数

    [参考文章-CSDN](https://blog.csdn.net/niaolianjiulin/article/details/76129578)
    
    存储够用时用快排；否则用桶排序；
2. 数组求top k
3. 求每组top 3
4. 多核CPU和多CPU区别

    [多核 CPU 和多个 CPU 有何区别？ - 卧闻海棠花的回答 - 知乎](https://www.zhihu.com/question/20998226/answer/18659825)
    
    多核CPU和多CPU的区别主要在于性能和成本
    
    如果我们选择多个单核CPU，那么每一个CPU都需要有较为独立的电路支持，有自己的Cache，而他们之间通过板上的总线进行通信。假如在这样的架构上，我们要跑一个多线程的程序（常见典型情况），不考虑超线程，那么每一个线程就要跑在一个独立的CPU上，线程间的所有协作都要走总线，而共享的数据更是有可能要在好几个Cache里同时存在。这样的话，总线开销相比较而言是很大的，怎么办？那么多Cache，即使我们不心疼存储能力的浪费，一致性怎么保证？如果真正做出来，还要在主板上占多块地盘，给布局布线带来更大的挑战，怎么搞定？
    
    如果我们选择多核单CPU，那么我们只需要一套芯片组，一套存储，多核之间通过芯片内部总线进行通信，共享使用内存。在这样的架构上，如果我们跑一个多线程的程序，那么线程间通信将比上一种情形更快。如果最终实现出来，对板上空间的占用较小，布局布线的压力也较小。
    
5. TCP的三次握手、四次挥手大致流程。Time_wait和close_wait是什么。拥塞控制和流量控制。
6. **操作系统的页式存储是怎样的。有什么优点和缺点。Linux的进程调度。**
7. 进程和线程的区别。
8. 怎么对10亿个数字进行排序
9. 如何把多个Hadoop集群连接起来
10. 数据库都有哪些引擎
11. 数据库的锁了解哪些，说说
12. 应用层协议有哪些
13. 什么场景用TCP，什么场景用UDP
14. HTTP状态码都有哪些，具体说一下
15. **HTTP长连接和短连接**

    [长连接与短连接](https://www.cnblogs.com/0201zcr/p/4694945.html)
    
    在HTTP/1.0中，默认使用的是短连接。也就是说，浏览器和服务器每进行一次HTTP操作，就建立一次连接，但任务结束就中断连接。如果客户端浏览器访问的某个HTML或其他类型的 Web页中包含有其他的Web资源，如JavaScript文件、图像文件、CSS文件等；当浏览器每遇到这样一个Web资源，就会建立一个HTTP会话。

    但从 HTTP/1.1起，默认使用长连接，用以保持连接特性。使用长连接的HTTP协议，会在响应头有加入这行代码：`Connection:keep-alive`

    在使用长连接的情况下，当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的 TCP连接不会关闭，如果客户端再次访问这个服务器上的网页，会继续使用这一条已经建立的连接。Keep-Alive不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如Apache）中设定这个时间。实现长连接要客户端和服务端都支持长连接。

    HTTP协议的长连接和短连接，实质上是TCP协议的长连接和短连接。
16. url和uri的区别

    [参考文章---简书](https://www.jianshu.com/p/ba15d066f777)
    
    `URI`：`Uniform Resource Identifier`，统一资源标识符
    
    `URL`：`Uniform Resource Location`统一资源定位符
17. 线程和进程的区别
18. **hashmap的实现**
19. **hashmap的扩容**
20. 一条直线可以把一个平面分成2个平面，两条直线最多分成4个，问n条直线最多分成几个面

    [一个直线将一个平面分成 2 部分，两条直线分成 4 部分](https://www.nowcoder.com/questionTerminal/6568b3d008174b11bcae238d4872770a)
    
21. TCP（连接 断开） UDP
22. 进程的用户栈和内核栈
23. **操作系统 同步IO 异步IO**
24. FIFO的缺点
25. 同步 异步 谈谈你的理解
26. 进程通信 线程通信

## 与字节跳动相关的大数据面经

[内推熊](https://mp.weixin.qq.com/s/fJ_Pg1tFwHRR5RoKpoP5VQ)
